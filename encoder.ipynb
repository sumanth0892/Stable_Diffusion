{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6dc44d89",
   "metadata": {},
   "source": [
    "### The Encoder is used to:\n",
    "- Add noise to the input images, if given and then obtain the latents for the diffusion model\n",
    "- As such, the encoder is used **ONLY WHEN** we give an input image, i.e during training and it isn't used during the rest of the times, mainly **during inference**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855372bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## The necessary imports\n",
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d123bae1",
   "metadata": {},
   "source": [
    "### The Encoder block is composed of:\n",
    "- The Attention block\n",
    "- The Residual Block"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e95e55",
   "metadata": {},
   "source": [
    "#### The Self-Attention and the Attention Blocks\n",
    "- The Attention block is made up of Self-Attention block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97fab0bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, n_heads, d_embed, in_proj_bias = True, out_proj_bias = True):\n",
    "        \"\"\"\n",
    "        Param n_heads: the number of heads in the attention block\n",
    "        Param d_embed: the embedding dimension of the token, i.e. the length of the vector for each token\n",
    "        Param in_proj_bias\n",
    "        Param out_proj_bias\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.in_proj = nn.Linear(d_embed, 3 * d_embed, bias = in_proj_bias)\n",
    "        self.out_proj = nn.Linear(d_embed, d_embed, bias = out_proj_bias)\n",
    "        self.n_heads = n_heads\n",
    "        self.d_head = d_embed // n_heads\n",
    "\n",
    "    def forward(self, x, causal_mask = False):\n",
    "        input_shape = x.shape\n",
    "        batch_size, sequence_length, d_embed = input_shape\n",
    "        interim_shape = (batch_size, sequence_length, self.n_heads, self.d_head)\n",
    "\n",
    "        q, k, v = self.in_proj(x).chunk(3, dim = -1)\n",
    "        q = q.view(interim_shape).transpose(1, 2)\n",
    "        k = k.view(interim_shape).transpose(1, 2)\n",
    "        v = v.view(interim_shape).transpose(1, 2)\n",
    "\n",
    "        weight = q @ k.transpose(-1, -2)\n",
    "        if causal_mask:\n",
    "            mask = torch.ones_like(weight, dtype = torch.bool).triu(1)\n",
    "            weight.masked_fill_(mask, -torch.inf)\n",
    "        weight /= math.sqrt(self.d_head)\n",
    "        weight = F.softmax(weight, dim = -1)\n",
    "\n",
    "        output = weight @ v\n",
    "        output = output.transpose(1, 2)\n",
    "        output = output.reshape(input_shape)\n",
    "        output = self.out_proj(output)\n",
    "        return output\n",
    "    \n",
    "class AttentionBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Inherits from the Module class of the nn class.\n",
    "    \"\"\"\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        self.groupnorm = nn.GroupNorm(32, channels)\n",
    "        self.attention = SelfAttention(1, channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residue = x\n",
    "        x = self.groupnorm(x)\n",
    "        n, c, h, w = x.shape\n",
    "        x = x.view((n, c, h*w))\n",
    "        x = x.transpose(-1, -2)\n",
    "        x = self.attention(x)\n",
    "        x = x.transpose(-1, -2)\n",
    "        x = x.view(n, c, h, w)\n",
    "        x += residue\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a55aa24",
   "metadata": {},
   "source": [
    "#### Residual Block\n",
    "- The Residual Block has skip connections to help with the problem of **Vanishing gradients** and add more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4979c8a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.groupnorm_1 = nn.GroupNorm(32, in_channels)\n",
    "        self.conv_1 = nn.Conv2d(in_channels, out_channels, kernel_size = 3, padding = 1)\n",
    "\n",
    "        self.groupnorm_2 = nn.GroupNorm(32, out_channels)\n",
    "        self.conv_2 = nn.Conv2d(out_channels, out_channels, kernel_size = 3, padding = 1)\n",
    "\n",
    "        if in_channels == out_channels:\n",
    "            self.residual_layer = nn.Identity()\n",
    "        else:\n",
    "            self.residual_layer = nn.Conv2d(in_channels, out_channels, kernel_size = 1, padding = 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residue = x\n",
    "        x = self.groupnorm_1(x)\n",
    "        x = F.silu(x)\n",
    "        x = self.conv_1(x)\n",
    "\n",
    "        x = self.groupnorm_2(x)\n",
    "        x = F.silu(x)\n",
    "        x = self.conv_2(x)\n",
    "\n",
    "        return x + self.residual_layer(residue)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea7b6b39",
   "metadata": {},
   "source": [
    "### The Encoder Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a83981",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    This module stacks Convolutional layers and Residual blocks to convert an input image into a representation of size (8, 8)\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Conv2d(3, 128, kernel_size = 3, padding = 1),\n",
    "            ResidualBlock(128, 128),\n",
    "            ResidualBlock(128, 128),\n",
    "            nn.Conv2d(128, 128, kernel_size = 3, stride = 2, padding = 0),\n",
    "            ResidualBlock(128, 256),\n",
    "            ResidualBlock(256, 256),\n",
    "            nn.Conv2d(256, 256, kernel_size = 3, stride = 2, padding = 0),\n",
    "            ResidualBlock(256, 512),\n",
    "            ResidualBlock(512, 512),\n",
    "            nn.Conv2d(512, 512, kernel_size = 3, stride = 2, padding = 0),\n",
    "            ResidualBlock(512, 512),\n",
    "            ResidualBlock(512, 512),\n",
    "            ResidualBlock(512, 512),\n",
    "            AttentionBlock(512),\n",
    "            ResidualBlock(512, 512),\n",
    "            nn.GroupNorm(32, 512), # Figure out what this does\n",
    "            nn.SiLU(),# Figure out what this does\n",
    "            nn.Conv2d(512, 8, kernel_size = 3, padding = 1),\n",
    "            nn.Conv2d(8, 8, kernel_size = 1, padding = 0)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, noise):\n",
    "        _, _, h, w = x.shape \n",
    "        for layer in self.layers:\n",
    "            if getattr(layer, 'stride', None) == (2, 2): # Padding at downsampling should be asymmetric\n",
    "                x = F.pad(x, (0, 1, 0, 1))\n",
    "            x = layer(x)\n",
    "        assert x.shape == torch.Size([1, 8, h//8, w//8])\n",
    "        print(f\"The shape of the representation after the convolutional layers and Residual blocks is {x.shape}\")\n",
    "\n",
    "        mean, log_variance = torch.chunk(x, 2, dim = 1)\n",
    "        log_variance = torch.clamp(log_variance, -30, 20)\n",
    "        variance = log_variance.exp()\n",
    "        stdev = variance.sqrt()\n",
    "        x = mean + stdev * noise\n",
    "        x *= 0.18215\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b81656c7",
   "metadata": {},
   "source": [
    "### The Inputs to the encoder are:\n",
    "- Input images tensor of size (B, C, H, W)\n",
    "- Encoder noise of shape (len(prompts), H//8, W//8\n",
    "- Thus, we see that an image of size (3, 512, 512) has been reduced to a representation of (8, 64, 64)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c305645",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Consider an input of one prompt\n",
    "H = W = 512\n",
    "## Generate the noise\n",
    "generator = torch.Generator(); noise_shape = (1, H//8, W//8)\n",
    "encoder_noise = torch.randn(noise_shape, generator = generator)\n",
    "input_images = torch.randint(0, 255, (1, 3, H, W), dtype = torch.float32)\n",
    "## Encode a sample image\n",
    "encoder = Encoder()\n",
    "latents = encoder(input_images, encoder_noise)\n",
    "assert latents.shape == torch.Size([1, 4, H//8, W//8])\n",
    "assert latents.dtype == torch.float32"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 kernel",
   "language": "python",
   "name": "python310_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
