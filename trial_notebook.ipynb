{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5dc357e7",
   "metadata": {},
   "source": [
    "## Use this notebook to learn about Stable Diffusion or Latent Diffusion Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec60bad5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## The necessary imports\n",
    "import util\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "#from google.colab import files\n",
    "# from stable_diffusion import util\n",
    "# from stable_diffusion import pipeline\n",
    "# from stable_diffusion import model_loader\n",
    "# models = model_loader.preload_models('cpu') # Assume the device is the CPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2011c5ac",
   "metadata": {},
   "source": [
    "### The Inference step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7aa5e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(prompts,\n",
    "            unconditional_prompts = None,\n",
    "            input_images = None,\n",
    "            strength = 0.8,\n",
    "            do_cfg = True,\n",
    "            cfg_scale = 7.5,\n",
    "            height = 512,\n",
    "            width = 512,\n",
    "            sampler = \"k_lms\",\n",
    "            n_inference_steps = 5,\n",
    "            models = None,\n",
    "            seed = None,\n",
    "            device = None,\n",
    "            idle_device = None):\n",
    "    if models is None:\n",
    "        models = {}\n",
    "    with torch.no_grad():\n",
    "        if not prompts or not isinstance(prompts, (list, tuple)):\n",
    "            raise ValueError(\"prompts must be a list or tuple\")\n",
    "        if unconditional_prompts and not isinstance(unconditional_prompts, (list, tuple)):\n",
    "            raise ValueError(\"unconditional prompts must be non-empty\")\n",
    "        # Ensure the length of unconditional prompts is the same as the input prompts\n",
    "        unconditional_prompts = unconditional_prompts or [\"\"] * len(prompts)\n",
    "        \n",
    "        # Check for input images\n",
    "        if input_images and not isinstance(unconditional_prompts, (list, tuple)):\n",
    "            raise ValueError(f\"{input_images} must be a non-empty list or tuple if provided\")\n",
    "        if input_images and len(prompts) != len(input_images):\n",
    "            raise ValueError(f\"Length of input_images {input_images} must be the same as the length of prompts {prompts}\")\n",
    "        if not 0 <= strength <= 1:\n",
    "            raise ValueError(f\"{strength} must be between 0 and 1\")\n",
    "        \n",
    "        # Check for the height and width dimensions\n",
    "        if height % 8 or width % 8:\n",
    "            raise ValueError(\"Height and Width must be multiples of 8\")\n",
    "            \n",
    "        # Check for the availability of device\n",
    "        if device is None:\n",
    "            device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        if idle_device:\n",
    "            to_idle = lambda x: x.to(idle_device)\n",
    "        else:\n",
    "            to_idle = lambda x: x\n",
    "        \n",
    "        # generator\n",
    "        generator = torch.generator(device=device)\n",
    "        if seed is None:\n",
    "            generator.seed()\n",
    "        else:\n",
    "            generator.manual_seed(seed)\n",
    "        \n",
    "        # Text tokenizer\n",
    "        tokenizer = Tokenizer()\n",
    "        clip = models.get(\"clip\") or model_loader.clip(device)\n",
    "        clip.to(device)\n",
    "        \n",
    "        # Use the datatype (dtype) of the model weights as the default dtype\n",
    "        dtype = clip.embedding.position_value.dtype\n",
    "        if do_cfg:\n",
    "            conditional_tokens = tokenizer.encode_batch(prompts)\n",
    "            conditional_tokens = torch.tensor(conditional_tokens, dtype=torch.long, device=device)\n",
    "            conditional_context = clip(conditional_tokens)\n",
    "            unconditional_tokens = tokenizer.encode_batch(unconditional_prompts)\n",
    "            unconditional_tokens = torch.tensor(unconditional_tokens, dtype=torch.long, device=device)\n",
    "            unconditional_context = clip(unconditional_tokens)\n",
    "            context = torch.cat([conditional_context, unconditional_context])\n",
    "        else:\n",
    "            tokens = tokenizer.encode_batch(prompts)\n",
    "            tokens = torch.tensor(tokens, dtype=torch.long, device=device)\n",
    "            context = clip(tokens)\n",
    "        to_idle(clip)\n",
    "        del tokenizer, clip\n",
    "        \n",
    "        # Obtain different samplers based on the input requirements\n",
    "        if sampler == \"k_lms\":\n",
    "            sampler = KLMSSampler(n_inference_steps=n_inference_steps)\n",
    "        elif sampler == \"k_euler\":\n",
    "            sampler = KEulerSampler(n_inference_steps=n_inference_steps)\n",
    "        elif sampler == \"k_euler_ancestral\":\n",
    "            sampler = KEulerAncestralSampler(n_inference_steps=n_inference_steps)\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\"Unknown sampler value\")\n",
    "\n",
    "        noise_shape = (len(prompts), 4, height//8, width//8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe363702",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = [\"a photograph of an astronaut riding a horse\"] #@param {type: \"string\"}\n",
    "prompts = [prompt]\n",
    "\n",
    "unconditional_prompt = \"\" #@param {type: \"string\"}\n",
    "unconditional_prompts = [unconditional_prompt] if unconditional_prompt else None\n",
    "\n",
    "upload_input_image = False #@param {type: \"boolean\"}\n",
    "input_images = None\n",
    "if upload_input_image:\n",
    "    print(\"Upload an input image\")\n",
    "    path = list(files.upload().keys())[0]\n",
    "    input_images = [Image.open(path)]\n",
    "\n",
    "strength = 0.8 #@param {type: \"slider\", min:0, max:1, step:0.01}\n",
    "do_cfg = True #@param {type: \"boolean\"}\n",
    "height = 512 #@param {type: \"integer\"}\n",
    "width = 512 #@param {type: \"integer\"}\n",
    "sampler = \"k_lms\" #@param [\"k_lms\", \"k_euler\", \"k_euler_ancestral\"]\n",
    "n_inference_steps = 50 #@param {type: \"integer\"}\n",
    "\n",
    "use_seed = False\n",
    "if use_seed:\n",
    "    seed = 42\n",
    "else:\n",
    "    seed = None\n",
    "\n",
    "## Generate the image based on the set of prompts given above\n",
    "images = generate(prompts=prompts, unconditional_prompts=unconditional_prompts,\n",
    "                  input_images=input_images, strength=strength,\n",
    "                  do_cfg=do_cfg, cfg_scale=cfg_scale,\n",
    "                  height=height, width=width, sampler=sampler,\n",
    "                  n_inference_steps=n_inference_steps, seed=seed,\n",
    "                  models=models, device='cpu', idle_device='cpu')[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b63cdc",
   "metadata": {},
   "source": [
    "### Sampler and Timesteps\n",
    "- A Sampler is used to generate noise for the decoding process and added to the latents.\n",
    "- There are different types of samplers based on performance criteria such as **speed**, **accuracy** and **image quality**.\n",
    "- Taking the inputs **n_inference_steps** and **n_training_steps**, create a timestep as **(n_training_steps//n_inference_steps)**.\n",
    "- For each of the above timesteps, obtain an **embedding** of a specified length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f59ca36",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KEulerSampler():\n",
    "\tdef __init__(self, n_inference_steps = 50, n_training_steps = 1000):\n",
    "\t\ttimesteps = np.linspace(n_training_steps - 1, 0, n_inference_steps)\n",
    "\t\talphas_cumprod = util.get_alphas_cumprod(n_training_steps = n_training_steps)\n",
    "\t\tsigmas = (1 - alphas_cumprod) / (alphas_cumprod) ** 0.5\n",
    "\t\tlog_sigmas = np.log(sigmas)\n",
    "\t\tlog_sigmas = np.interp(timesteps, range(n_training_steps), log_sigmas)\n",
    "\t\tsigmas = np.exp(log_sigmas)\n",
    "\t\tsigmas = np.append(sigmas, 0)\n",
    "\n",
    "\t\tself.sigmas = sigmas\n",
    "\t\tself.initial_scale = sigmas.max()\n",
    "\t\tself.timesteps = timesteps\n",
    "\t\tself.n_inference_steps = n_inference_steps\n",
    "\t\tself.n_training_steps = n_training_steps\n",
    "\t\tself.step_count = 0\n",
    "\n",
    "\tdef get_input_scale(self, step_count = None):\n",
    "\t\tif step_count is None:\n",
    "\t\t\tstep_count = self.step_count\n",
    "\t\tsigma = self.sigmas[step_count]\n",
    "\t\treturn 1 / (sigma ** 2 + 1) ** 0.5\n",
    "\n",
    "\tdef set_strength(self, strength = 1):\n",
    "\t\tstart_step = self.n_inference_steps - int(self.n_inference_steps * strength)\n",
    "\t\tself.timesteps = np.linspace(self.n_training_steps - 1, 0, self.n_inference_steps)\n",
    "\t\tself.timesteps = self.timesteps[start_step:]\n",
    "\t\tself.initial_scale = self.sigmas[start_step]\n",
    "\t\tself.step_count = start_step\n",
    "\n",
    "\tdef step(self, latents, output):\n",
    "\t\tt = self.step_count\n",
    "\t\tself.step_count += 1\n",
    "\t\tsigma_from = self.sigmas[t]\n",
    "\t\tsigma_to = self.sigmas[t + 1]\n",
    "\t\tlatents += output * (sigma_to - sigma_from)\n",
    "\t\treturn latents"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 kernel",
   "language": "python",
   "name": "python310_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
