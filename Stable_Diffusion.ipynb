{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "874c3518",
   "metadata": {},
   "source": [
    "## Diffusion Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9468d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "## The necessary imports\n",
    "import math\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from tqdm import tqdm\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1b634e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_time_embedding(timestep, dtype = torch.float32):\n",
    "    \"\"\"\n",
    "    Takes a timestep and gives the embedding\n",
    "    \"\"\"\n",
    "    freqs = torch.pow(10000, -torch.arange(start = 0, end = 160, dtype = dtype) / 160)\n",
    "    x = torch.tensor([timestep], dtype = dtype)[:, None] * freqs[None]\n",
    "    return torch.cat([torch.cos(x), torch.sin(x)], dim = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe821208",
   "metadata": {},
   "source": [
    "#### Time Embedding module for the vector in the attention mechansim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1868815b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeEmbedding(nn.Module):\n",
    "    def __init__(self, n_embed):\n",
    "        \"\"\"\n",
    "        n_embed: Embedding vector dimension\n",
    "        Linear layers to add more parameters for learning.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.linear_1 = nn.Linear(n_embed, 4*n_embed)\n",
    "        self.linear_1 = nn.Linear(4*n_embed, 4*n_embed)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.linear_1(x)\n",
    "        x = F.silu(x)\n",
    "        x = self.linear_2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c940408",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "\tdef __init__(self, n_heads, d_embed, in_proj_bias = True, out_proj_bias = True):\n",
    "\t\t\"\"\"\n",
    "\t\tParam n_heads: the number of heads in the attention block\n",
    "\t\tParam d_embed: the embedding dimension of the token, i.e. the length of the vector for each token\n",
    "\t\tParam in_proj_bias\n",
    "\t\tParam out_proj_bias\n",
    "\t\t\"\"\"\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.in_proj = nn.Linear(d_embed, 3 * d_embed, bias = in_proj_bias)\n",
    "\t\tself.out_proj = nn.Linear(d_embed, d_embed, bias = out_proj_bias)\n",
    "\t\tself.n_heads = n_heads\n",
    "\t\tself.d_head = d_embed // n_heads\n",
    "\n",
    "\tdef forward(self, x, causal_mask = False):\n",
    "\t\tinput_shape = x.shape\n",
    "\t\tbatch_size, sequence_length, d_embed = input_shape\n",
    "\t\tinterim_shape = (batch_size, sequence_length, self.n_heads, self.d_head)\n",
    "\n",
    "\t\tq, k, v = self.in_proj(x).chunk(3, dim = -1)\n",
    "\t\tq = q.view(interim_shape).transpose(1, 2)\n",
    "\t\tk = k.view(interim_shape).transpose(1, 2)\n",
    "\t\tv = v.view(interim_shape).transpose(1, 2)\n",
    "\n",
    "\t\tweight = q @ k.transpose(-1, -2)\n",
    "\t\tif causal_mask:\n",
    "\t\t\tmask = torch.ones_like(weight, dtype = torch.bool).triu(1)\n",
    "\t\t\tweight.masked_fill_(mask, -torch.inf)\n",
    "\t\tweight /= math.sqrt(self.d_head)\n",
    "\t\tweight = F.softmax(weight, dim = -1)\n",
    "\n",
    "\t\toutput = weight @ v\n",
    "\t\toutput = output.transpose(1, 2)\n",
    "\t\toutput = output.reshape(input_shape)\n",
    "\t\toutput = self.out_proj(output)\n",
    "\t\treturn output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e57cc18",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossAttention(nn.Module):\n",
    "    def __init__(self, n_heads, d_embed, d_cross, in_proj_bias=True, out_proj_bias=True):\n",
    "        super().__init__()\n",
    "        self.q_proj   = nn.Linear(d_embed, d_embed, bias=in_proj_bias)\n",
    "        self.k_proj   = nn.Linear(d_cross, d_embed, bias=in_proj_bias)\n",
    "        self.v_proj   = nn.Linear(d_cross, d_embed, bias=in_proj_bias)\n",
    "        self.out_proj = nn.Linear(d_embed, d_embed, bias=out_proj_bias)\n",
    "        self.n_heads = n_heads\n",
    "        self.d_head = d_embed // n_heads\n",
    "    \n",
    "    def forward(self, x, y):\n",
    "        input_shape = x.shape\n",
    "        batch_size, sequence_length, d_embed = input_shape\n",
    "        interim_shape = (batch_size, -1, self.n_heads, self.d_head)\n",
    "\n",
    "        q = self.q_proj(x)\n",
    "        k = self.k_proj(y)\n",
    "        v = self.v_proj(y)\n",
    "\n",
    "        q = q.view(interim_shape).transpose(1, 2)\n",
    "        k = k.view(interim_shape).transpose(1, 2)\n",
    "        v = v.view(interim_shape).transpose(1, 2)\n",
    "\n",
    "        weight = q @ k.transpose(-1, -2)\n",
    "        weight /= math.sqrt(self.d_head)\n",
    "        weight = F.softmax(weight, dim=-1)\n",
    "\n",
    "        output = weight @ v\n",
    "        output = output.transpose(1, 2).contiguous()\n",
    "        output = output.view(input_shape)\n",
    "        output = self.out_proj(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3e40c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, n_time = 1280):\n",
    "        super().__init__()\n",
    "        self.groupnorm_feature = nn.GroupNorm(32, in_channels)\n",
    "        self.conv_feature = nn.Conv2d(in_channels, out_channels, kernel_size = 3, padding = 1)\n",
    "        self.linear_time = nn.Linear(n_time, out_channels)\n",
    "        \n",
    "        self.groupnorm_merged = nn.GroupNorm(32, out_channels)\n",
    "        self.conv_merged = nn.Conv2d(out_channels, out_channels, kernel_size = 3, padding = 1)\n",
    "        \n",
    "        if in_channels == out_channels:\n",
    "            self.residual_layer = nn.Identity()\n",
    "        else:\n",
    "            self.residual_layer = nn.Conv2d(in_channels, out_channels, kernel_size = 1, padding = 0)\n",
    "    \n",
    "    def forward(self, feature, time):\n",
    "        residue = feature\n",
    "        \n",
    "        feature = self.groupnorm_feature(feature)\n",
    "        feature = F.silu(feature)\n",
    "        feature = self.conv_feature(feature)\n",
    "        \n",
    "        time = F.silu(time)\n",
    "        time = self.linear_time(time)\n",
    "        \n",
    "        merged = feature + time.unsqueeze(-1).unsqueeze(-1)\n",
    "        merged = self.groupnorm_merged(merged)\n",
    "        merged = F.silu(merged)\n",
    "        merged = self.conv_merged(merged)\n",
    "        \n",
    "        return merged + self.residual_layer(residue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb820b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionBlock(nn.Module):\n",
    "\tdef __init__(self, n_heads, n_embed, d_context = 768):\n",
    "\t\tsuper().__init__()\n",
    "\t\tchannels = n_heads * n_embed\n",
    "\n",
    "\t\tself.groupnorm = nn.GroupNorm(32, channels, eps = 1e-6)\n",
    "\t\tself.conv_input = nn.Conv2d(channels, channels, kernel_size = 1, padding = 0)\n",
    "\n",
    "\t\tself.layernorm_1 = nn.LayerNorm(channels)\n",
    "\t\tself.attention_1 = SelfAttention(n_heads, channels, in_proj_bias = False)\n",
    "\t\tself.layernorm_2 = nn.LayerNorm(channels)\n",
    "\t\tself.attention_2 = CrossAttention(n_heads, channels, d_context, in_proj_bias = False)\n",
    "\t\tself.layernorm_3 = nn.LayerNorm(channels)\n",
    "\t\tself.linear_geglu_1 = nn.Linear(channels, 4*channels*2)\n",
    "\t\tself.linear_geglu_2 = nn.Linear(4 * channels, channels)\n",
    "\n",
    "\t\tself.conv_output = nn.Conv2d(channels, channels, kernel_size = 1, padding = 0)\n",
    "\n",
    "\tdef forward(self, x, context):\n",
    "\t\tresidue_long = x\n",
    "\n",
    "\t\tx = self.groupnorm(x)\n",
    "\t\tx = self.conv_input(x)\n",
    "\n",
    "\t\tn, c, h, w = x.shape\n",
    "\t\tx = x.view((n, c, h*w)) # Transforms to size (n, c, hw)\n",
    "\t\tx = x.transpose(-1, -2) # Reorder to (n, hw, c)\n",
    "\n",
    "\t\tresidue_short = x\n",
    "\t\tx = self.layernorm_1(x)\n",
    "\t\tx = self.attention_1(x)\n",
    "\t\tx += residue_short\n",
    "\n",
    "\t\tresidue_short = x\n",
    "\t\tx = self.layernorm_2(x)\n",
    "\t\tx = self.attention_2(x, context)\n",
    "\t\tx += residue_short\n",
    "\n",
    "\t\tresidue_short = x\n",
    "\t\tx = self.layernorm_3(x)\n",
    "\t\tx, gate = self.linear_geglu_1(x).chunk(2, dim = -1)\n",
    "\t\tx = x * F.gelu(gate)\n",
    "\t\tx = self.linear_geglu_2(x)\n",
    "\t\tx += residue_short\n",
    "\n",
    "\t\tx = x.transpose(-1, -2) # Reorder to (n, c, hw)\n",
    "\t\tx = x.view(n, c, h, w) # (Reshape to n, c, h, w)\n",
    "\t\treturn self.conv_output(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c9824e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Upsample(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(channels, channels, kernel_size = 3, padding = 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.interpolate(x, scale_factor = 2, mode = \"nearest\")\n",
    "        return self.conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c54bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwitchSequential(nn.Sequential):\n",
    "    def forward(self, x, context, time):\n",
    "        for layer in self:\n",
    "            if isinstance(layer, AttentionBlock):\n",
    "                x = layer(x, context)\n",
    "            elif isinstance(layer, ResidualBlock):\n",
    "                x = layer(x, time)\n",
    "            else:\n",
    "                x = layer(time)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a030ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoders = nn.ModuleList([\n",
    "            SwitchSequential(nn.Conv2d(4, 320, kernel_size = 3, padding = 1)),\n",
    "            SwitchSequential(ResidualBlock(320, 320), AttentionBlock(8, 40)),\n",
    "            SwitchSequential(ResidualBlock(320, 320), AttentionBlock(8, 40)),\n",
    "            SwitchSequential(nn.Conv2d(320, 320, kernel_size = 3, stride = 2, padding = 1)),\n",
    "            SwitchSequential(ResidualBlock(320, 640), AttentionBlock(8, 80)),\n",
    "            SwitchSequential(ResidualBlock(640, 640), AttentionBlock(8, 80)),\n",
    "            SwitchSequential(nn.Conv2d(640, 640, kernel_size = 3, stride = 2, padding = 1)),\n",
    "            SwitchSequential(ResidualBlock(640, 1280), AttentionBlock(8, 160)),\n",
    "            SwitchSequential(ResidualBlock(640, 1280), AttentionBlock(8, 160)),\n",
    "            SwitchSequential(nn.Conv2d(1280, 1280, kernel_size = 3, stride = 2, padding = 1)),\n",
    "            SwitchSequential(ResidualBlock(1280, 1280)),\n",
    "            SwitchSequential(ResidualBlock(1280, 1280)),\n",
    "        ])\n",
    "    \n",
    "        self.bottleneck = nn.ModuleList([\n",
    "            SwitchSequential(ResidualBlock(1280, 1280)),\n",
    "            SwitchSequential(AttentionBlock(8, 160)),\n",
    "            SwitchSequential(ResidualBlock(1280, 1280)),\n",
    "        ])\n",
    "        \n",
    "        self.decoders = nn.ModuleList([\n",
    "            SwitchSequential(ResidualBlock(2560, 1280)),\n",
    "            SwitchSequential(ResidualBlock(2560, 1280)),\n",
    "            SwitchSequential(ResidualBlock(2560, 1280), Upsample(1280)),\n",
    "            SwitchSequential(ResidualBlock(2560, 1280), AttentionBlock(8, 160)),\n",
    "            SwitchSequential(ResidualBlock(2560, 1280), AttentionBlock(8, 160)),\n",
    "            SwitchSequential(ResidualBlock(1920, 1280), AttentionBlock(8, 160), Upsample(1280)),\n",
    "            SwitchSequential(ResidualBlock(1920, 640), AttentionBlock(8, 80)),\n",
    "            SwitchSequential(ResidualBlock(1280, 640), AttentionBlock(8, 80)),\n",
    "            SwitchSequential(ResidualBlock(960, 640), AttentionBlock(8, 80), Upsample(640)),\n",
    "            SwitchSequential(ResidualBlock(960, 320), AttentionBlock(8, 40)),\n",
    "            SwitchSequential(ResidualBlock(640, 320), AttentionBlock(8, 40)),\n",
    "            SwitchSequential(ResidualBlock(640, 320), AttentionBlock(8, 40)),\n",
    "        ])\n",
    "    \n",
    "    def forward(self, x, context, time):\n",
    "        skip_connections = []\n",
    "        for layers in self.encoders:\n",
    "            x = layers(x, context, time)\n",
    "            skip_connections.append(x)\n",
    "        x = self.bottleneck(x, context, time)\n",
    "        \n",
    "        for layers in self.decoders:\n",
    "            x = torch.cat((x, skip_connections.pop()), dim = 1)\n",
    "            x = layers(x, context, time)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c80e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FinalLayer(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.groupnorm = nn.GroupNorm(32, in_channels)\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size = 3, padding = 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.groupnorm(x)\n",
    "        x = F.silu(x)\n",
    "        x = self.conv(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f8a55ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Diffusion(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.time_embedding = TimeEmbedding(320)\n",
    "        self.unet = UNet()\n",
    "        self.final = FinalLayer(320, 4)\n",
    "    \n",
    "    def forward(self, latent, context, time):\n",
    "        time = self.time_embedding(time)\n",
    "        output = self.unet(latent, context, time)\n",
    "        output = self.final(output)\n",
    "        print(f\"The shape of the output from the diffusion module is {output.shape}\")\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50a5be1",
   "metadata": {},
   "source": [
    "#### To simulate how the Diffusion module works, we need:\n",
    "- Timesteps from the sampler used to generate noise\n",
    "- The time_embedding from the utils\n",
    "- Context from the CLIP Module, i.e. the context derived from the input text prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65be8eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = torch.randn(1, 77, 768, dtype = torch.float32) # Derived by adding context to the input text prompts\n",
    "latents = torch.randn(1, 4, 64, 64) # Derived from encoding the input images\n",
    "time_embedding = torch.randn(1, 320, dtype = torch.float32) # Derived from the sampler\n",
    "diffusion = Diffusion()\n",
    "input_latents = latents * 0.386"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "323ef5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## First the TimeEmbedding Module\n",
    "## One of the inputs to the UNet is the time embedding of shape (320 * 4), if 320 is the embedding dimension\n",
    "linear_1 = nn.Linear(320, 320 * 4) # 320 as the embedding dimension\n",
    "linear_2 = nn.Linear(320 * 4, 320 * 4)\n",
    "time = linear_1(time_embedding)\n",
    "assert time.shape == torch.Size([1, 1280])\n",
    "time = F.silu(time)\n",
    "assert time.shape == torch.Size([1, 1280])\n",
    "time = linear_2(time)\n",
    "assert time.shape == torch.Size([1, 1280])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eebd4849",
   "metadata": {},
   "source": [
    "##### Next comes the Time-varying UNet module\n",
    "- Encoder block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99061977",
   "metadata": {},
   "outputs": [],
   "source": [
    "## After the time embedding, the inputs are run through the UNet\n",
    "## The encoder is composed of ResidualBlocks, AttentionBlocks and Convolution layers\n",
    "x = input_latents; skip_connections = []\n",
    "print(f\"Prior to the UNet module, the input latents are of shape {x.shape}\")\n",
    "print(f\"Prior to the UNet module, the context is of shape {context.shape}\")\n",
    "print(f\"Prior to the UNet module, the time-embedding input is of shape {time.shape}\")\n",
    "## Let us run through the encoder block of the UNet first\n",
    "# Starts with a Convolution layer\n",
    "x = nn.Conv2d(4, 320, kernel_size = 3, padding = 1)(x)\n",
    "assert x.shape == torch.Size([1, 320, 64, 64])\n",
    "skip_connections.append(x)\n",
    "# This is followed by a ResidualBlock and an AttentionBlock\n",
    "x = ResidualBlock(320, 320)(x, time)\n",
    "assert x.shape == torch.Size([1, 320, 64, 64])\n",
    "skip_connections.append(x)\n",
    "x = AttentionBlock(8, 40)(x, context)\n",
    "assert x.shape == torch.Size([1, 320, 64, 64])\n",
    "skip_connections.append(x)\n",
    "# This is also followed by a ResidualBlock and an AttentionBlock\n",
    "x = ResidualBlock(320, 320)(x, time)\n",
    "assert x.shape == torch.Size([1, 320, 64, 64])\n",
    "skip_connections.append(x)\n",
    "x = AttentionBlock(8, 40)(x, context)\n",
    "assert x.shape == torch.Size([1, 320, 64, 64])\n",
    "skip_connections.append(x)\n",
    "# Next comes a Convolution layer to understand the features of the input map\n",
    "x = nn.Conv2d(320, 320, kernel_size = 3, stride = 2, padding = 1)(x)\n",
    "assert x.shape == torch.Size([1, 320, 32, 32])\n",
    "skip_connections.append(x)\n",
    "x = ResidualBlock(320, 640)(x, time)\n",
    "assert x.shape == torch.Size([1, 640, 32, 32])\n",
    "skip_connections.append(x)\n",
    "x = AttentionBlock(8, 80)(x, context)\n",
    "assert x.shape == torch.Size([1, 640, 32, 32])\n",
    "skip_connections.append(x)\n",
    "x = nn.Conv2d(640, 640, kernel_size = 3, stride = 2, padding = 1)(x)\n",
    "assert x.shape == torch.Size([1, 640, 16, 16])\n",
    "skip_connections.append(x)\n",
    "x = ResidualBlock(640, 1280)(x, time)\n",
    "assert x.shape == torch.Size([1, 1280, 16, 16])\n",
    "skip_connections.append(x)\n",
    "x = AttentionBlock(8, 160)(x, context)\n",
    "assert x.shape == torch.Size([1, 1280, 16, 16])\n",
    "skip_connections.append(x)\n",
    "x = nn.Conv2d(1280, 1280, kernel_size = 3, stride = 2, padding = 1)(x)\n",
    "assert x.shape == torch.Size([1, 1280, 8, 8])\n",
    "skip_connections.append(x)\n",
    "x = ResidualBlock(1280, 1280)(x, time)\n",
    "assert x.shape == torch.Size([1, 1280, 8, 8])\n",
    "skip_connections.append(x)\n",
    "x = ResidualBlock(1280, 1280)(x, time)\n",
    "assert x.shape == torch.Size([1, 1280, 8, 8])\n",
    "skip_connections.append(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0960aa25",
   "metadata": {},
   "source": [
    "##### Time-varying UNet\n",
    "- Bottleneck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "331a911a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Consider the layers of the bottleneck\n",
    "print(f\"At the start of the bottleneck, the inputs are of shape {x.shape}\")\n",
    "## We have two ResidualBlocks and one Attention mechanism\n",
    "x = ResidualBlock(1280, 1280)(x, time)\n",
    "assert x.shape == torch.Size([1, 1280, 8, 8])\n",
    "x = AttentionBlock(8, 160)(x, context)\n",
    "assert x.shape == torch.Size([1, 1280, 8, 8])\n",
    "x = ResidualBlock(1280, 1280)(x, time)\n",
    "assert x.shape == torch.Size([1, 1280, 8, 8])\n",
    "x = AttentionBlock(8, 160)(x, context)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b5ed36",
   "metadata": {},
   "source": [
    "##### Time-varying UNet\n",
    "- Decoder Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e20315",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The skip connections are as follows:\")\n",
    "for s_c in skip_connections:\n",
    "    print(type(s_c))\n",
    "    print(s_c.shape)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "660ced02",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Consider the layers of the decoder block\n",
    "print(f\"At the start of the decoder block, the inputs are of shape {x.shape}\")\n",
    "x0 = skip_connections.pop()\n",
    "print(f\"The skip connection is of shape {x0.shape}\")\n",
    "print(f\"The input is of shape {x.shape}\")\n",
    "x = torch.cat((x, x0), dim = 1)\n",
    "print(f\"The Concatenated input is of shape {x.shape}\")\n",
    "assert x.shape == torch.Size([1, 2560, 8, 8])\n",
    "print(f\"After concatenation, the input is of shape {x.shape}\")\n",
    "x = ResidualBlock(2560, 1280)(x, time)\n",
    "assert x.shape == torch.Size([1, 1280, 8, 8])\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "print(f\"There are {len(skip_connections)} skip connections left\")\n",
    "x0 = skip_connections.pop()\n",
    "print(f\"The skip connection is of shape {x0.shape}\")\n",
    "print(f\"The input is of shape {x.shape}\")\n",
    "x = torch.cat((x, x0), dim = 1)\n",
    "print(f\"The Concatenated input is of shape {x.shape}\")\n",
    "assert x.shape == torch.Size([1, 2560, 8, 8])\n",
    "print(f\"After concatenation, the input is of shape {x.shape}\")\n",
    "x = ResidualBlock(2560, 1280)(x, time)\n",
    "assert x.shape == torch.Size([1, 1280, 8, 8])\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "print(f\"There are {len(skip_connections)} skip connections left\")\n",
    "x0 = skip_connections.pop()\n",
    "print(f\"The skip connection is of shape {x0.shape}\")\n",
    "print(f\"The input is of shape {x.shape}\")\n",
    "x = torch.cat((x, x0), dim = 1)\n",
    "print(f\"The Concatenated input is of shape {x.shape}\")\n",
    "assert x.shape == torch.Size([1, 2560, 8, 8])\n",
    "print(f\"After concatenation, the input is of shape {x.shape}\")\n",
    "x = ResidualBlock(2560, 1280)(x, time)\n",
    "assert x.shape == torch.Size([1, 1280, 8, 8])\n",
    "x = Upsample(1280)(x)\n",
    "assert x.shape == torch.Size([1, 1280, 16, 16])\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "print(f\"There are {len(skip_connections)} skip connections left\")\n",
    "x0 = skip_connections.pop()\n",
    "print(f\"The skip connection is of shape {x0.shape}\")\n",
    "print(f\"The input is of shape {x.shape}\")\n",
    "x = torch.cat((x, x0), dim = 1)\n",
    "print(f\"The Concatenated input is of shape {x.shape}\")\n",
    "assert x.shape == torch.Size([1, 2560, 16, 16])\n",
    "print(f\"After concatenation, the input is of shape {x.shape}\")\n",
    "x = ResidualBlock(2560, 1280)(x, time)\n",
    "assert x.shape == torch.Size([1, 1280, 16, 16])\n",
    "x = AttentionBlock(8, 160)(x, context)\n",
    "assert x.shape == torch.Size([1, 1280, 16, 16])\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "print(f\"There are {len(skip_connections)} skip connections left\")\n",
    "x0 = skip_connections.pop()\n",
    "print(f\"The skip connection is of shape {x0.shape}\")\n",
    "print(f\"The input is of shape {x.shape}\")\n",
    "x = torch.cat((x, x0), dim = 1)\n",
    "print(f\"The Concatenated input is of shape {x.shape}\")\n",
    "assert x.shape == torch.Size([1, 2560, 16, 16])\n",
    "print(f\"After concatenation, the input is of shape {x.shape}\")\n",
    "x = ResidualBlock(2560, 1280)(x, time)\n",
    "assert x.shape == torch.Size([1, 1280, 16, 16])\n",
    "x = AttentionBlock(8, 160)(x, context)\n",
    "assert x.shape == torch.Size([1, 1280, 16, 16])\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "print(f\"There are {len(skip_connections)} skip connections left\")\n",
    "x0 = skip_connections.pop()\n",
    "print(f\"The skip connection is of shape {x0.shape}\")\n",
    "print(f\"The input is of shape {x.shape}\")\n",
    "x = torch.cat((x, x0), dim = 1)\n",
    "print(f\"The Concatenated input is of shape {x.shape}\")\n",
    "assert x.shape == torch.Size([1, 1920, 16, 16])\n",
    "print(f\"After concatenation, the input is of shape {x.shape}\")\n",
    "x = ResidualBlock(1920, 1280)(x, time)\n",
    "assert x.shape == torch.Size([1, 1280, 16, 16])\n",
    "x = AttentionBlock(8, 160)(x, context)\n",
    "assert x.shape == torch.Size([1, 1280, 16, 16])\n",
    "x = Upsample(1280)(x)\n",
    "assert x.shape == torch.Size([1, 1280, 32, 32])\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "print(f\"There are {len(skip_connections)} skip connections left\")\n",
    "x0 = skip_connections.pop()\n",
    "print(f\"The skip connection is of shape {x0.shape}\")\n",
    "print(f\"The input is of shape {x.shape}\")\n",
    "x = torch.cat((x, x0), dim = 1)\n",
    "print(f\"The Concatenated input is of shape {x.shape}\")\n",
    "assert x.shape == torch.Size([1, 1920, 32, 32])\n",
    "x = ResidualBlock(1920, 640)(x, time)\n",
    "assert x.shape == torch.Size([1, 640, 32, 32])\n",
    "x = AttentionBlock(8, 80)(x, context)\n",
    "assert x.shape == torch.Size([1, 640, 32, 32])\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "print(f\"There are {len(skip_connections)} skip connections left\")\n",
    "x0 = skip_connections.pop()\n",
    "print(f\"The skip connection is of shape {x0.shape}\")\n",
    "print(f\"The input is of shape {x.shape}\")\n",
    "x = torch.cat((x, x0), dim = 1)\n",
    "print(f\"The Concatenated input is of shape {x.shape}\")\n",
    "assert x.shape == torch.Size([1, 1280, 32, 32])\n",
    "x = ResidualBlock(1280, 640)(x, time)\n",
    "assert x.shape == torch.Size([1, 640, 32, 32])\n",
    "x = AttentionBlock(8, 80)(x, context)\n",
    "assert x.shape == torch.Size([1, 640, 32, 32])\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "print(f\"There are {len(skip_connections)} skip connections left\")\n",
    "x0 = skip_connections.pop()\n",
    "print(f\"The skip connection is of shape {x0.shape}\")\n",
    "print(f\"The input is of shape {x.shape}\")\n",
    "x = torch.cat((x, x0), dim = 1)\n",
    "print(f\"The Concatenated input is of shape {x.shape}\")\n",
    "assert x.shape == torch.Size([1, 960, 32, 32])\n",
    "x = ResidualBlock(960, 640)(x, time)\n",
    "assert x.shape == torch.Size([1, 640, 32, 32])\n",
    "x = AttentionBlock(8, 80)(x, context)\n",
    "assert x.shape == torch.Size([1, 640, 32, 32])\n",
    "x = Upsample(640)(x)\n",
    "assert x.shape == torch.Size([1, 640, 64, 64])\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "print(f\"There are {len(skip_connections)} skip connections left\")\n",
    "x0 = skip_connections.pop()\n",
    "print(f\"The skip connection is of shape {x0.shape}\")\n",
    "print(f\"The input is of shape {x.shape}\")\n",
    "x = torch.cat((x, x0), dim = 1)\n",
    "print(f\"The Concatenated input is of shape {x.shape}\")\n",
    "assert x.shape == torch.Size([1, 960, 64, 64])\n",
    "x = ResidualBlock(960, 320)(x, time)\n",
    "assert x.shape == torch.Size([1, 320, 64, 64])\n",
    "x = AttentionBlock(8, 40)(x, context)\n",
    "assert x.shape == torch.Size([1, 320, 64, 64])\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "print(f\"There are {len(skip_connections)} skip connections left\")\n",
    "x0 = skip_connections.pop()\n",
    "print(f\"The skip connection is of shape {x0.shape}\")\n",
    "print(f\"The input is of shape {x.shape}\")\n",
    "x = torch.cat((x, x0), dim = 1)\n",
    "print(f\"The Concatenated input is of shape {x.shape}\")\n",
    "assert x.shape == torch.Size([1, 640, 64, 64])\n",
    "x = ResidualBlock(640, 320)(x, time)\n",
    "assert x.shape == torch.Size([1, 320, 64, 64])\n",
    "x = AttentionBlock(8, 40)(x, context)\n",
    "assert x.shape == torch.Size([1, 320, 64, 64])\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "print(f\"There are {len(skip_connections)} skip connections left\")\n",
    "x0 = skip_connections.pop()\n",
    "print(f\"The skip connection is of shape {x0.shape}\")\n",
    "print(f\"The input is of shape {x.shape}\")\n",
    "x = torch.cat((x, x0), dim = 1)\n",
    "print(f\"The Concatenated input is of shape {x.shape}\")\n",
    "assert x.shape == torch.Size([1, 640, 64, 64])\n",
    "x = ResidualBlock(640, 320)(x, time)\n",
    "assert x.shape == torch.Size([1, 320, 64, 64])\n",
    "x = AttentionBlock(8, 40)(x, context)\n",
    "assert x.shape == torch.Size([1, 320, 64, 64])\n",
    "\n",
    "print(\"\\n\")\n",
    "print(f\"After the UNet, the final output is of shape {x.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fbdc415",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"There are {len(skip_connections)} skip connections left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0393e3ee",
   "metadata": {},
   "source": [
    "#### Final Layer\n",
    "- The Final layer is a convolution conversion to a lower dimension map for the Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37c53c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = nn.GroupNorm(32, 320)(x)\n",
    "assert x.shape == torch.Size([1, 320, 64, 64])\n",
    "x = F.silu(x)\n",
    "assert x.shape == torch.Size([1, 320, 64, 64])\n",
    "x = nn.Conv2d(320, 4, kernel_size = 3, padding = 1)(x)\n",
    "assert x.shape == torch.Size([1, 4, 64, 64])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
