{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67630d2f",
   "metadata": {},
   "source": [
    "## Samplers\n",
    "- Samplers are used to generate noise according to a given number of inference steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2249ff25",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Necessary imports\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973baf53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_alphas_cumprod(beta_start = -0.00085, beta_end = 0.0120, n_training_steps = 1000):\n",
    "    betas = np.linspace(beta_start ** 0.5, beta_end ** 0.5, n_training_steps, dtype = np.float32) ** 2\n",
    "    print(f\"The betas have the shape {betas.shape}\")\n",
    "    alphas = 1.0 - betas\n",
    "    alphas_cumprod = np.cumprod(alphas, axis = 0)\n",
    "    print(f\"The alphas cumprod have the shape {alphas_cumprod.shape}\")\n",
    "    return alphas_cumprod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ba03e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_time_embedding(timestep, dtype):\n",
    "\t\"\"\"\n",
    "\tTakes a timestep as an input and gives the embedding\n",
    "\tLook at this in the notebook\n",
    "\t\"\"\"\n",
    "\tfreqs = torch.pow(10000, -torch.arange(start = 0, end = 160, dtype = dtype) / 160)\n",
    "\tx = torch.tensor([timestep], dtype = dtype)[:, None] * freqs[None]\n",
    "\treturn torch.cat([torch.cos(x), torch.sin(x)], dim = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d9f5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KEulerSampler():\n",
    "\tdef __init__(self, n_inference_steps = 50, n_training_steps = 1000, lms_order = 4):\n",
    "\t\ttimesteps = np.linspace(n_training_steps - 1, 0, n_inference_steps)\n",
    "\t\talphas_cumprod = get_alphas_cumprod(n_training_steps = n_training_steps)\n",
    "\t\tsigmas = (1 - alphas_cumprod) / (alphas_cumprod) ** 0.5\n",
    "\t\tlog_sigmas = np.log(sigmas)\n",
    "\t\tlog_sigmas = np.interp(timesteps, range(n_training_steps), log_sigmas)\n",
    "\t\tsigmas = np.exp(log_sigmas)\n",
    "\t\tsigmas = np.append(sigmas, 0)\n",
    "\n",
    "\t\tself.sigmas = sigmas\n",
    "\t\tself.initial_scale = sigmas.max()\n",
    "\t\tself.timesteps = timesteps\n",
    "\t\tself.n_inference_steps = n_inference_steps\n",
    "\t\tself.n_training_steps = n_training_steps\n",
    "\t\tself.lms_order = lms_order\n",
    "\t\tself.step_count = 0\n",
    "\t\tself.outputs = []\n",
    "\n",
    "\tdef get_input_scale(self, step_count = None):\n",
    "\t\tif step_count is None:\n",
    "\t\t\tstep_count = self.step_count\n",
    "\t\tsigma = self.sigmas[step_count]\n",
    "\t\treturn 1 / (sigma ** 2 + 1) ** 0.5\n",
    "\n",
    "\tdef set_strength(self, strength = 1):\n",
    "\t\tstart_step = self.n_inference_steps - int(self.n_inference_steps * strength)\n",
    "\t\tself.timesteps = np.linspace(self.n_training_steps - 1, 0, self.n_inference_steps)\n",
    "\t\tself.timesteps = self.timesteps[start_step:]\n",
    "\t\tself.initial_scale = self.sigmas[start_step]\n",
    "\t\tself.step_count = start_step\n",
    "\n",
    "\tdef step(self, latents, output):\n",
    "\t\tt = self.step_count\n",
    "\t\tself.step_count += 1\n",
    "\n",
    "\t\tself.outputs = [output] + self.outputs[:self.lms_order - 1]\n",
    "\t\torder = len(self.outputs)\n",
    "\n",
    "\t\tfor i, output in enumerate(self.outputs):\n",
    "\t\t\t# Integrate the polynomial by a trapezoidal approximation method for 81 points\n",
    "\t\t\tx = np.linspace(self.sigmas[t], self.sigmas[t + 1], 81)\n",
    "\t\t\ty = np.ones(81)\n",
    "\t\t\tfor j in range(order):\n",
    "\t\t\t\tif i == j:\n",
    "\t\t\t\t\tcontinue\n",
    "\t\t\t\ty *= x - self.sigmas[t - j]\n",
    "\t\t\t\ty /= self.sigmas[t - i] - self.sigmas[t - j]\n",
    "\t\t\tlms_coeff = np.trapz(y = y, x = x)\n",
    "\t\t\tlatents += lms_coeff * output\n",
    "\t\treturn latents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8799fd68",
   "metadata": {},
   "source": [
    "### Using the sampler\n",
    "- Get the sampler\n",
    "- set the strength of the sampler, the higher the value the stronger the image generated but slower the inference\n",
    "- Multiply the latents with the sampler's initial scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0daa8612",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = KEulerSampler()\n",
    "sampler.set_strength(0.8)\n",
    "type(sampler.initial_scale)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139e8dcc",
   "metadata": {},
   "source": [
    "### Latents and Samplers\n",
    "- Once we get the latents either from the encoder or our own (if input images are not given), we multiply them with the sampler's initial scale.\n",
    "- Using the context from the CLIP Embedding, Time embedding and the input latents, obtain the output of the diffusion model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee537a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler.initial_scale\n",
    "## Consider a sample latents and multiply them with the sampler's initial scale\n",
    "latents = torch.randn(1, 4, 64, 64)\n",
    "latents *= sampler.initial_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e79f917",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get the timesteps and the input latents\n",
    "timesteps = tqdm(sampler.timesteps)\n",
    "for _, timestep in enumerate(timesteps):\n",
    "    time_embedding = get_time_embedding(timestep, dtype = torch.float32)\n",
    "    input_latents = latents * sampler.get_input_scale()\n",
    "assert input_latents.shape == torch.Size([1, 4, 64, 64])\n",
    "## If the parameter \"do conditional guidance\" is enabled\n",
    "input_latents = input_latents.repeat(2, 1, 1, 1)\n",
    "assert input_latents.shape == torch.Size([2, 4, 64, 64])\n",
    "assert input_latents.dtype == torch.float32\n",
    "print(torch.min(input_latents))\n",
    "print(torch.max(input_latents))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
