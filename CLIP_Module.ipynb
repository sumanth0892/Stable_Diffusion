{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c93bf4c9",
   "metadata": {},
   "source": [
    "## CLIP Embedding\n",
    "- The Clip Embedding takes a bunch of tokens (the text prompt input) and adds context to them\n",
    "- The CLIP module is composed of the **CLIPEmbedding** and **CLIPlayer** modules for the job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1f5175",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156b18ea",
   "metadata": {},
   "source": [
    "### The Self-Attention module is an essential component of the CLIP Embedding module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca3aa85",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "\tdef __init__(self, n_heads, d_embed, in_proj_bias = True, out_proj_bias = True):\n",
    "\t\t\"\"\"\n",
    "\t\tParam n_heads: the number of heads in the attention block\n",
    "\t\tParam d_embed: the embedding dimension of the token, i.e. the length of the vector for each token\n",
    "\t\tParam in_proj_bias\n",
    "\t\tParam out_proj_bias\n",
    "\t\t\"\"\"\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.in_proj = nn.Linear(d_embed, 3 * d_embed, bias = in_proj_bias)\n",
    "\t\tself.out_proj = nn.Linear(d_embed, d_embed, bias = out_proj_bias)\n",
    "\t\tself.n_heads = n_heads\n",
    "\t\tself.d_head = d_embed // n_heads\n",
    "\n",
    "\tdef forward(self, x, causal_mask = False):\n",
    "\t\tinput_shape = x.shape\n",
    "\t\tbatch_size, sequence_length, d_embed = input_shape\n",
    "\t\tinterim_shape = (batch_size, sequence_length, self.n_heads, self.d_head)\n",
    "\n",
    "\t\tq, k, v = self.in_proj(x).chunk(3, dim = -1)\n",
    "\t\tq = q.view(interim_shape).transpose(1, 2)\n",
    "\t\tk = k.view(interim_shape).transpose(1, 2)\n",
    "\t\tv = v.view(interim_shape).transpose(1, 2)\n",
    "\n",
    "\t\tweight = q @ k.transpose(-1, -2)\n",
    "\t\tif causal_mask:\n",
    "\t\t\tmask = torch.ones_like(weight, dtype = torch.bool).triu(1)\n",
    "\t\t\tweight.masked_fill_(mask, -torch.inf)\n",
    "\t\tweight /= math.sqrt(self.d_head)\n",
    "\t\tweight = F.softmax(weight, dim = -1)\n",
    "\n",
    "\t\toutput = weight @ v\n",
    "\t\toutput = output.transpose(1, 2)\n",
    "\t\toutput = output.reshape(input_shape)\n",
    "\t\toutput = self.out_proj(output)\n",
    "\t\treturn output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e02735b0",
   "metadata": {},
   "source": [
    "### The CLIPEmbedding module converts the tokens into tensors with embeddings and adds position values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b59d2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLIPEmbedding(nn.Module):\n",
    "\tdef __init__(self, n_vocab: int, n_embed: int, n_tokens: int):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.token_embedding = nn.Embedding(n_vocab, n_embed)\n",
    "\t\tself.position_value = nn.Parameter(torch.zeros(n_tokens, n_embed))\n",
    "\n",
    "\tdef forward(self, tokens):\n",
    "\t\tx = self.token_embedding(tokens)\n",
    "\t\tx += self.position_value\n",
    "\t\treturn x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced90de6",
   "metadata": {},
   "source": [
    "### CLIPlayer module uses the Self-Attention module to add context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075b46b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLIPlayer(nn.Module):\n",
    "    def __init__(self, n_heads: int, n_embed: int):\n",
    "        super().__init__()\n",
    "        self.layernorm_1 = nn.LayerNorm(n_embed)\n",
    "        self.attention = SelfAttention(n_heads, n_embed)\n",
    "        self.layernorm_2 = nn.LayerNorm(n_embed)\n",
    "        self.linear_1 = nn.Linear(n_embed, 4 * n_embed)\n",
    "        self.linear_2 = nn.Linear(4 * n_embed, n_embed)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layernorm_1(x)\n",
    "        x = self.attention(x, causal_mask = True)\n",
    "        residue = x\n",
    "        x = self.layernorm_2(x)\n",
    "        x = self.linear_1(x)\n",
    "        x = x * torch.sigmoid(1.702 * x)\n",
    "        x = self.linear_2(x)\n",
    "        x += residue\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e164c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLIP(nn.Module):\n",
    "\tdef __init__(self):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.embedding = CLIPEmbedding(49408, 768, 77)\n",
    "\t\tself.layers = nn.ModuleList([\n",
    "\t\t\tCLIPlayer(12, 768) for i in range(12)\n",
    "\t\t\t])\n",
    "\t\tself.layernorm = nn.LayerNorm(768)\n",
    "\n",
    "\tdef forward(self, tokens: torch.LongTensor) -> torch.FloatTensor:\n",
    "\t\ttokens = tokens.type(torch.long)\n",
    "\t\tstate = self.embedding(tokens)\n",
    "\t\tfor layer in self.layers:\n",
    "\t\t\tstate = layer(state)\n",
    "\t\toutput = self.layernorm(state)\n",
    "\t\treturn output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02991ead",
   "metadata": {},
   "source": [
    "#### The Embeddings for the CLIP Module, i.e. the Embedding module named CLIPEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c794bc62",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create an Embedding module to convert the tokens into embedding tensors\n",
    "## The layers are as follows:\n",
    "## Here the vocabulary size is 49408, i.e. the number of words in the dictionary\n",
    "## The size of the embedding vector, i.e. the size by which each token is represented is 768\n",
    "## The number of tokens, i.e. the number of words in the query is fixed at 77 (random number)\n",
    "token_embedding = nn.Embedding(49408, 768)\n",
    "position_value = nn.Parameter(torch.zeros(77, 768))\n",
    "## Consider a set of input tokens\n",
    "input_tokens = torch.randint(10, 100, (1, 77))\n",
    "## First, obtain the embeddings for these tokens and then input the positions for the embeddings to derive context\n",
    "token_embeddings = token_embedding(input_tokens)\n",
    "assert token_embeddings.shape == torch.Size([1, 77, 768])\n",
    "token_embeddings += position_value\n",
    "assert token_embeddings.shape == torch.Size([1, 77, 768])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "736d738c",
   "metadata": {},
   "source": [
    "#### Next, the CLIP Layers to derive context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd297172",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_layer = CLIPlayer(12, 768)\n",
    "state = sample_layer(token_embeddings)\n",
    "assert state.shape == torch.Size([1, 77, 768])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b14b03b",
   "metadata": {},
   "source": [
    "#### Finally a layer normalization to enable robust learnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28a9f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_norm = nn.LayerNorm(768)\n",
    "context = layer_norm(state)\n",
    "assert context.shape == torch.Size([1, 77, 768])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
